---
layout: page
title: '<br><h1 class="title">Video Turing Test: <br>Toward Human-Level Video Story Understanding</h1>'
subtitle: '<h4 class="subtitle">@E4, COEX Convention Center, ICCV 2019<br>Half day (AM) on Nov 2, 2019, Seoul, Korea</h4><br><a class="btn btn-default" href="http://iccv2019.thecvf.com/"><img src="/assets/img/workshop/ICCV19logo_main.png" style="max-width: 9em; padding: 0.1em 0.1em; background-color:#FFFFFF;"></a><br><br>'
permalink: /Workshop/2019
feature-img: "assets/img/main.png"
---

<link rel="stylesheet" href="/assets/css/member.css">

<style>
    table {
        width: 100%;
    }
    
    div.content-container{
        position: relative;
        background-image: url(/assets/img/workshop/main.png);                                                               
        height: 400px;
        background-size: cover;
        z-index: 200;
    }
    
    span{
        text-indent: 10px;
    }
</style>

<div class="content-subcontainer" style="TEXT-ALIGN: left; color:black" id="top">
  <h2> 1. Call for Papers </h2>
  <p> The ability to craft and understand stories is a crucial cognitive tool used by humans for communication. According to computational linguists, narrative theorists and cognitive scientists, the story understanding is a good proxy to measure the readers' intelligence. Readers can understand a story as a way of problem-solving in which, for example, they keep focusing on how main characters overcome certain obstacles throughout the story. Readers need to make inferences both in prospect and in retrospect about the causal relationships between different events in the story. </p>
  <p> Especially, the video story data such as TV shows and movies can serve as an excellent testbed to evaluate the human-level AI algorithms from two points of view. First, video data have different modalities such as a sequence of images, audios (including dialogue, sound effects and background music) and text (subtitles or added comments). Second, video data show various cross-sections of everyday life. Therefore, understanding video story can be thought of a significant challenge to current AI technology, which involves analyzing and simulating human vision, language, thinking, and behavior.  </p>
  <p> Towards human-level video understanding, machine intelligence needs to extract meaningful information such as events from the sequential multimodal video data, consider the causal relationships between different events, and make inferences both in prospect and in retrospect about what events will occur and how these events could occur. Story in the video is highly-abstracted information which consists of a series of events across multiple scenes in a scenario. </p>
  <p> In this workshop, we emphasize the necessity of findings and insights from the various research domain for video story understanding. We aims to invite experts in variety of related fields, including vision, language processing, computational narratology and neuro-symbolic computing to provide a perspective on the research that exists, and initiates discussion of future challenges in data-driven video understanding. Topics of interest include but not limited to:</p>
  <ul>
    <li>Deep learning architecture for multi-modal video story representation</li>
    <li>Question answering about video story</li>
    <li>Summarization and retrieval from long story video contents</li>
    <li>Scene description generation for video understanding</li>
    <li>Scene graph generation and relationship detection from video</li>
    <li>Activity/Event recognition from video</li>
    <li>Character identification &amp; interaction modeling in video</li>
    <li>Emotion recognition in video</li>
    <li>Novel tasks about video understanding and challenge dataset</li>
  </ul>
  This workshop will invite a selected set of leading researchers in the related fields for invited talks. Also, we encourage submissions of papers as extended abstract within 4 pages. 
  <h3> Submission Instructions </h3>
  <p>We invite submissions of papers as <b>extended abstract within 4 pages</b>, excluding references or supplementary materials. All submissions must be in pdf format as a single file (incl. supplementary materials) using below templates and submitted through <a href="https://cmt3.research.microsoft.com/VTT2019">this CMT link</a>. 
  The review process is single-round and double-blind. All submissions have to be anonymized. </p>
  <ul>
    <li> LaTeX/Word Templates (tar): <a href="http://iccv2019.thecvf.com/files/iccv2019AuthorKit.tgz">iccv2019AuthorKit.tgz</a> </li>
  </ul>
  <p> All accepted papers will be presented as posters during the workshop and listed on the website. 
    Additionally, a small number of accepted papers will be selected to be presented as contributed talks. </p>
  <h3> Dual Submissions </h3>
  <p>Note that this workshop will not publish official proceedings. The accepted submission will not be counted as a publication. We encourage submissions of relevant work that has been previously published, or is to be presented at the main conference.</p>
</div> <br />

<div class="content-subcontainer" style="TEXT-ALIGN: left; color:black">
  <h2> 2. Important Dates </h2>
  <table>
      <tr>	
        <th style="width: 350px">Paper Submission Deadline</th>
        <td style="width: 400px">September 10, 2019 (GMT+9) &nbsp;&nbsp;<button type="button" class="btn btn-danger">Extended</button></td>
      </tr>
      <tr>	
        <th>Notification to Authors</th>
        <td>October 7, 2019</td>
      </tr>
      <tr>	
        <th>Paper Camera-Ready Deadline</th>
        <td>October 18, 2019</td>
      </tr>
      <tr>	
        <th>Workshop Date</th>
        <td>November 2, 2019</td>
      </tr>
  </table>
</div> <br />


<div class="content-subcontainer" style="TEXT-ALIGN: left; color:black">
  <h2> 3. Schedule </h2>
  <h4>Chair: Chang Dong Yoo (KAIST)</h4>
  <table class="table table-striped">
    <thead>
      <tr>
        <th scope="col" style="width: 150px">Time</th>
        <th scope="col">Presentation</th>
        <!--<th scope="col">Presenter</th>-->
      </tr>
    </thead>
    <tbody>
      <tr>	
        <th scope="row">08:30 - 08:45</th>
        <td>Opening Remarks: <b>Video Turing Test</b>, 
        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a> (Seoul National University) </td>
      </tr>
      <tr>
        <th scope="row">08:45 - 09:15</th>
        <td>Invited Talk 1: <b>Video Understanding: Action, Activities and Beyond</b>,
        <a href="https://www.cs.ubc.ca/~lsigal/">Leonid Sigal</a> (University of British Columbia)</td>
      </tr>
      <tr>
        <th scope="row">09:15 - 09:45</th>
        <td>Invited Paper 1: <br><b>VideoMem: Constructing, Analyzing, Predicting Short-Term and Long-Term Video Memorability </b> <br>Romain Cohendet, Claire-Hélène Demarty, Ngoc Q. K. Duong, Martin Engilberge <br><br>
        Invited Paper 2: <br><b>Progressive Attention Memory Network for Movie Story Question Answering </b> <br>Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, Chang D. Yoo <br><br>
        Invited Paper 3: <br><b>HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips </b> <br>Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic</td>
        <!--<td>Speaker</td>-->
      </tr>
      <tr>
        <th scope="row">09:45 - 10:15</th>
        <td>Spotlight Talks (5 minutes each) <br><br>
            <ol start="1" style="line-height: 1.2;">
                <li>
                <b>DIFRINT: Deep Iterative Frame Interpolation for Full-frame Video Stabilization </b> <br>Jinsoo Choi, In So Kweon <br>
                </li>
                <li>
                <b>Adversarial Inference for Multi-Sentence Video Description </b> <br>Jae Sung Park, Marcus Rohrbach, Trevor Darrell, Anna Rohrbach <br>
                </li>
                <li>
                <b>Robust Person Re-identification via Graph Convolution Networks </b> <br>Guisik Kim, Dongwook Shu, Junseok Kwon <br>
                </li>
                <li>
                <b>Enhancing Performance of Character Identification on Multiparty Dialogues of Drama via Multimodality </b> <br>Donghwan Kim <br>
                </li>
                <li>
                <b>Dual Attention Networks for Visual Reference Resolution in Visual Dialog </b> <br>Gi-Cheon Kang, Jaeseo Lim, Byoung-Tak Zhang <br>
                </li>
                <li>
                <b>Event Structure Frame-Annotated WordNet for Multimodal Inferencing </b> <br>Seohyun Im
                </li>
            </ol>
        </td>
      </tr>
      <tr>
        <th scope="row">10:15 - 11:25</th>
        <td>Coffee Break &amp; Poster Session</td>
        <!--<td>Speaker</td>-->
      </tr>
      <tr>
        <th scope="row">11:25 - 11:55</th>
        <td>Invited Talk 2: <b>High-level Video Understanding via Adversarial Inference and Spatio-temporal Graphs</b>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> (University of California, Berkeley) </td>
      </tr>
      <tr>
        <th scope="row">11:55 - 12:25</th>
        <td>Invited Talk 3: <b>Video Recognition from a Story</b>, 
        <a href="http://www.ceessnoek.info/">Cees Snoek</a> (University of Amsterdam) </td>
      </tr>
      <tr>
        <th scope="row">12:25 - 12:30</th>
        <td>Closing</td>
        <!--<td>Organizer</td>-->
      </tr>
    </tbody>
  </table>
</div> <br />



<div class="content-subcontainer" style="TEXT-ALIGN: left; color:black">
  <h2> 4. Invited Speakers </h2>
    <div class="col-md-12">
      <p>Invited Speaker 1: <a href="https://www.cs.ubc.ca/~lsigal/">Leonid Sigal</a>, University of British Columbia
      <br><b>Title: Video Understanding: Action, Activities and Beyond</b><br>
      Abstract: <span style="font-size: medium; ">Automatic understanding and interpretation of videos is one of the core challenges in computer vision. Many real-world systems could benefit from various levels of human and non-human video "action" understanding. In this talk, I will discuss some of the approaches we developed over the years for addressing aspects of this challenging problem. In particular, I will first discuss a strategy for learning activity progression in LSTM models, using structured rank losses, which explicitly encourage the architecture to increase its confidence in prediction over time. The resulting model turns out to be especially effective in early action detection. I will then talk about some of our recent work on single-frame situational recognition. Situational recognition goes beyond traditional action and activity understanding, which only focuses on detection of salient actions. Situational recognition further requires recognition of semantic roles for each action. For example, who is performing the action, what is the source and/or target of the action. We propose a mixture-kernel attention Graph Neural Network (GNN) for addressing this problem. Finally, time permitting, I will also discuss our recent work on audio-visual weakly-supervised dense video-captioning.</span>
      </p><br>
    </div>
    <div class="col-md-12">
      <p>Invited Speaker 2: <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>, University of California, Berkeley
      <br><b>Title: High-level Video Understanding via Adversarial Inference and Spatio-temporal Graphs</b><br>
      Abstract: <span style="font-size: medium; ">In this talk I'll present recent work towards High-level Video Understanding, including results on multi-sentence video description using novel adversarial inference methods.  Our adversarial method relies on a hybrid discriminator design, with constituent elements for linguistic coherence, visual relevance, and paragraph consistency.   I'll also present models for few-shot video activity recognition, leveraging scene graphs defined over space and time.  Our method targets activity recognition where labeled training data are expensive and rare, i.e. in few shot conditions, such as crash detection for autonomous driving.  Time permitting I'll cover other ongoing efforts on video description and activity recognition.</span>
      </p><br>
    </div>
    <div class="col-md-12">
      <p>Invited Speaker 3: <a href="http://www.ceessnoek.info/">Cees Snoek</a>, University of Amsterdam
      <br><b>Title: Video Recognition from a Story</b><br>
      Abstract: <span style="font-size: medium; ">By 2022 there will be 45 billion cameras in the world, many of them tiny, connected and live streaming 24/7. Self-driving cars, drones and service robots are just three manifestations. For all these applications it will be of critical importance to understand what is happening where and when in the video streams. The common tactic to spatiotemporal video recognition is to track a human-specified box or to learn a deep classification network from a set of predefined action classes. In this talk I will present an alternative approach, that allows for spatiotemporal recognition from a natural language sentence as input, and show its potential for object tracking and action segmentation. For object tracking, rather than specifying the target in the first frame of a video by a bounding box, a natural language specification of the target provides a more natural human-machine interaction as well as a means to improve tracking results. For action segmentation, rather than learning to segment from a fixed vocabulary of actor and action pairs, inference from a natural language input sentence allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are completely outside of the vocabulary. For both tasks we discuss the realization via multimodal network architectures and sentence-augmented datasets, comparisons with the traditional state-of-the-art, as well as their potential for application in surveillance and other live video streams.</span>
      </p>
    </div>
</div> <br />


<div class="content-subcontainer" style="TEXT-ALIGN: left; color:black">
  <h2> 5. Accepted papers </h2>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>DIFRINT: Deep Iterative Frame Interpolation for Full-frame Video Stabilization</b> <p style="font-size: medium; ">&nbsp;(spotlight)</p> <br>    
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Jinsoo Choi (KAIST); In So Kweon (KAIST) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>Adversarial Inference for Multi-Sentence Video Description </b> <p style="font-size: medium; ">&nbsp;(spotlight)</p> <br> 
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Jae Sung Park (UC Berkeley); Marcus Rohrbach (Facebook AI Research); Trevor Darrell (UC Berkeley); Anna Rohrbach (UC Berkeley) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>Robust Person Re-identification via Graph Convolution Networks </b> <p style="font-size: medium; ">&nbsp;(spotlight)</p> <br> 
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Guisik Kim (Chung-Ang Univ., Korea); Dongwook Shu (Chung-Ang Univ., Korea); Junseok Kwon (Chung-Ang Univ., Korea) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>Enhancing Performance of Character Identification on Multiparty Dialogues of Drama via Multimodality </b> <p style="font-size: medium; ">&nbsp;(spotlight)</p> <br>
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Donghwan Kim (Korea Advanced Institute of Science and Technology) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>Dual Attention Networks for Visual Reference Resolution in Visual Dialog </b> <p style="font-size: medium; ">&nbsp;(spotlight)</p> <br> 
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Gi-Cheon Kang (Seoul National University); Jaeseo Lim (Seoul National University); Byoung-Tak Zhang (Seoul National University) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>Event Structure Frame-Annotated WordNet for Multimodal Inferencing </b> <p style="font-size: medium; ">&nbsp;(spotlight)</p> <br> 
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Seohyun Im (Seoul National University) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>A Neural Question-Answering Manager for Video Question Answering </b> <br> 
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> A-Yeong Kim (Kyungpook National University); Gyu-Min Park (Kyung Hee University); Su-Hwan Yun (Kyung Hee University); Seong-Bae Park (Kyung Hee University) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>Emotion-based Story Event Clustering </b> <br> 
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Hye-Yeon Yu (Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea); Seohui Park (Sungkyunkwan University); Yun-Gyung Cheong (Sungkyunkwan University); Byung-Chull Bae (Hongik University) </span> <br>
    </blockquote>
  </li>
  <li style="padding-left: 1.3em; text-indent:-1.3em;">
    <b>Tripartite Heterogeneous Graph Propagation for Multimodal Entity Interaction Prediction </b> <br> 
    <blockquote style="padding-left: 0em; text-indent:0em;">
    <span style="font-size: medium; "> Kyung-Min Kim (Clova AI Research), Donghyun Kwak (Search Solution Inc.), Hanock Kwak (LINE Plus Copr.), Young-Jin Park (Naver R&D Center), Sangkwon Sim (Clova AI Research), Jae-Han Cho (Clova AI Research), Minkyu Kim (Clova AI Research), Jihun Kwon (Naver R&D Center), Nako Sung (Clova AI Research), Jung-Woo Ha (Clova AI Research) </span> <br>
    </blockquote>
  </li>
</div> <br />


<div class="content-subcontainer">
    <h2 class = "content-title" style="TEXT-ALIGN: left; color: black;">
      6. Organizers
    </h2>
    <div class="content-item" style="TEXT-ALIGN: center;">
      {% for person in site.data.people_workshop %}
        <div class="member" style="vertical-align: top; width: 250px;">
          <div class="member-profile">
            <img class="member-profile" src="{{person.src}}" alt="person">
          </div>
          <div class="member-name member-name">
            {{ person.name }}
          </div>
          <div class="member-info member-position">
            {{ person.affiliation }}
          </div>
        </div>
      {% endfor %}
    </div>
</div> <br />


<div class="content-subcontainer" style="TEXT-ALIGN: left; color:black">
<h2> 7. Program Committee </h2>
<ul>
  <li>Prof. Bohyung Han (Seoul National University)</li>
  <li>Prof. Byung-Chull Bae (Hongik University)</li>
  <li>Dr. Eun-Sol Kim (Kakao Brain)</li>
  <li>Prof. In So Kweon (KAIST)</li>
  <li>Prof. Ji-Hwan Kim (Sogang University)</li>
  <li>Dr. Jin-Hwa Kim (SK Telecom)</li>
  <li>Dr. Jung-Woo Ha (Naver)</li>
  <li>Prof. Junmo Kim (KAIST)</li>
  <li>Prof. Junseok Kwon (Chung-Ang University)</li>
  <li>Prof. Kristen Grauman (University of Texas at Austin)</li>
  <li>Dr. Kyung-Min Kim (Naver)</li>
  <li>Prof. Seon Joo Kim (Yonsei University)</li>
  <li>Prof. Seong-Bae Park (Kyung Hee University)</li>
  <li>Prof. Tamara L. Berg (UNC Chapel Hill) </li>
</ul>
</div> <br />

<div class="content-subcontainer" style="TEXT-ALIGN: center; color:black">
<footer>
  <p class="pull-right"><a href="#top">Back to top</a></p>
  <p>© 2019 Video Intelligence Center @ Seoul National University </p>
</footer>
